{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of data wrangling has first and foremost beaten into me the understanding of uncertainty and flawed design. Although I spent hours designing code that would programmatically fix my dataset into the format I envisioned, a single set of variation could result in the loss of large portion of vital data. I understand this is not exactly related to the specific process, but this realization was humbling.\n",
    "Of the three datasets, obtaining the twitter_api dataset was by far the most time consuming and confusing. Upon opening the initial twitter-archive-enhanged.csv I noticed that the expanded_urls column was unformatted and messy. This lead me to conclude that the data should be double checked with the source twitter data. However, I discovered that each json object was structured differently, and although I could access the minimum information, such as tweet_id, recounts, and fvaorties, expanded urls were often hidden within multiple layers of dictionaries. This led me to design the find_key() function, that would allow me to browse through dictionaries of arbitrary length and depth to retrieve the values of anything that matched the desired key. Although this was a pleasant success, I later scrapped the entire idea since I realized that expanded urls were meta-data that was unnecessary for the actual analysis, in addition to the fact that I could not determine the most useful expanded_url nor the best way to format twitter-archive-enhanced's expanded url column. Eventually I concluded that defining the columns most necessary for my analysis is a vital step in the data-wrangling process.\n",
    "While cleaning the twitter-archive-enhanced csv a large portion of the data wrangling process took place in programmatically discovering the errors. Although visual checks, .info(), and .describe() provided basic overviews of the dataset, the information was far from comprehensive. Instead, I began to think of contingencies that could occur and attempted to check for them. For instance, in the categorical variables, I imagined what would occur if a dog was misclassified or had been missed being classified, and proceeded to think of a solution to check the database for accuracy. This approach would repeat itself for the name column and for checking the numerator and denominator. When finding these errors, I would flag down the index in order to records its position during the cleaning process.\n",
    "The portion of the cleaning process that took the most time was accurately using regex to find the patterns needed. To be frank, I borrowed, read, and tested a lot of different regex until I found the pattern that worked, and even then I cannot personally guarantee the accuracy for every single observation.\n",
    "The last notable instance during the data wrangling project was during the analysis portion. I was trying to create a human-readable dataset that would rank the breed’s “popularity” using the concat() method. However, I discovered what appears to be a bug, as I scoured documentation and forums for answers, and left a note in the comments above the code. It would seem that the concat function does not always store the sorting of the datasets it is concatenating. I apologize if this is not a bug and rather my own lack of ability. \n",
    "Overall, this was a terribly trying and difficult project that really gave me a scope of how messy, tedious, and frustrating data wrangling can be (I fear that the current dataset is probably already quite friendly). However, I have to say, it was pretty satisfying when everything ran."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
